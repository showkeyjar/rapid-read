import streamlit as st
import json
import logging
import time
import os
from PyPDF2 import PdfReader
import io
from io import BytesIO
import requests
import random
import concurrent.futures
from functools import partial, lru_cache
import re
import hashlib
import threading

# ç¡®ä¿UTF-8æ—¥å¿—å¤„ç†
class UTF8StreamHandler(logging.StreamHandler):
    def emit(self, record):
        try:
            msg = self.format(record)
            if not isinstance(msg, str):
                msg = str(msg)
            stream = self.stream
            stream.write(msg.encode('utf-8').decode('utf-8') + self.terminator)
            self.flush()
        except Exception:
            self.handleError(record)

# Ensure logs directory exists
os.makedirs("logs", exist_ok=True)

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/app.log', encoding='utf-8'),
        UTF8StreamHandler()  # æ›¿æ¢åŸæ¥çš„StreamHandler
    ]
)
logger = logging.getLogger(__name__)

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ é…ç½®
MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
REQUEST_TIMEOUT = 120  # 120ç§’
MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
CHUNK_SIZE = 1024 * 1024  # 1MB
MAX_PAGES_PER_REQUEST = 10  # æ¯æ¬¡è¯·æ±‚å¤„ç†çš„æœ€å¤§é¡µæ•°
MAX_WORKERS = 12  # æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´
OLLAMA_TIMEOUT = 180  # 3åˆ†é’Ÿè¶…æ—¶
OLLAMA_RETRY_DELAY = 10  # é‡è¯•é—´éš”(ç§’)

def process_pdf(file) -> dict:
    """å¤„ç†PDFæ–‡ä»¶å¹¶è¿”å›æ–‡æœ¬å†…å®¹"""
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        contents = file.getvalue()
        
        # è§£æPDF
        pdf_reader = PdfReader(BytesIO(contents))
        page_count = len(pdf_reader.pages)
        logger.info(f"Processing PDF with {page_count} pages")
        
        # æå–æ–‡æœ¬
        text_pages = []
        for i, page in enumerate(pdf_reader.pages):
            text = page.extract_text() or ""
            text_pages.append(text)
            
        return {
            "text": "\n".join(text_pages),
            "pages": page_count
        }
        
    except Exception as e:
        logger.error(f"PDFå¤„ç†å¤±è´¥: {str(e)}")
        raise

# å…¨å±€æ¨¡å‹ç®¡ç†å™¨
class ModelManager:
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._model_loaded = False
            cls._instance._lock = threading.Lock()
        return cls._instance
    
    def ensure_model_loaded(self, provider: str):
        """å…ˆæ£€æŸ¥æœåŠ¡å¥åº·å†åŠ è½½æ¨¡å‹"""
        if not check_ollama_health():
            raise RuntimeError("OllamaæœåŠ¡å“åº”ç¼“æ…¢æˆ–ä¸å¯ç”¨")
        
        if self._model_loaded:
            return True
            
        with self._lock:
            if not self._model_loaded:
                if provider == "ollama" and not self._load_ollama_model():
                    raise RuntimeError("æ¨¡å‹åŠ è½½å¤±è´¥")
                self._model_loaded = True
        return True
    
    def _load_ollama_model(self) -> bool:
        """å®é™…åŠ è½½Ollamaæ¨¡å‹"""
        try:
            model_name = os.getenv('OLLAMA_MODEL')
            resp = requests.post(
                f"{os.getenv('OLLAMA_API_BASE')}/api/generate",
                json={"model": model_name, "prompt": ""},
                timeout=60
            )
            return resp.status_code == 200
        except Exception as e:
            logger.error(f"æ¨¡å‹é¢„çƒ­å¤±è´¥: {str(e)}")
            return False

# åœ¨è¿›ç¨‹å¯åŠ¨æ—¶åˆå§‹åŒ–
model_manager = ModelManager()

# æ·»åŠ LLMç›¸å…³å‡½æ•°
def generate_summary(text: str, level: int, provider: str) -> str:
    """ä¼˜åŒ–ç‰ˆæ‘˜è¦ç”Ÿæˆ(é€Ÿåº¦ä¼˜å…ˆ)"""
    prompt_templates = {
        1: "è¯·ç”¨ä¸­æ–‡ç®€æ´æ€»ç»“ä»¥ä¸‹æ–‡æœ¬çš„å…³é”®å†…å®¹(é™300å­—):\n{text}",
        2: "è¯·ç”¨ä¸­æ–‡æ¦‚æ‹¬ä»¥ä¸‹ç« èŠ‚çš„æ ¸å¿ƒè§‚ç‚¹(é™200å­—):\n{text}",
        3: "è¯·ç”¨3-5å¥ä¸­æ–‡æ€»ç»“å…¨æ–‡ä¸»æ—¨:\n{text}"
    }
    
    # æ›´ä¸¥æ ¼çš„é•¿åº¦é™åˆ¶
    max_input_len = {1: 8000, 2: 5000, 3: 3000}[level]
    text = text[:max_input_len]
    
    try:
        if provider.lower() == "ollama":
            response = requests.post(
                f"{os.getenv('OLLAMA_API_BASE')}/api/generate",
                json={
                    "model": os.getenv("OLLAMA_MODEL"),
                    "prompt": prompt_templates[level].format(text=text),
                    "options": {
                        "temperature": 0.5,  # æé«˜åˆ›é€ æ€§
                        "num_ctx": 2048,     # å‡å°ä¸Šä¸‹æ–‡çª—å£
                        "timeout": 60000,    # 1åˆ†é’Ÿè¶…æ—¶
                        "num_predict": 300   # é™åˆ¶è¾“å‡ºé•¿åº¦
                    },
                    "stream": True
                },
                stream=True,
                timeout=90  # 1.5åˆ†é’Ÿ
            )
            
            # å¤„ç†æµå¼å“åº”
            full_response = ""
            for line in response.iter_lines():
                if line:
                    chunk = json.loads(line)
                    full_response += chunk.get("response", "")
                    
                    # æ£€æŸ¥æ˜¯å¦å®Œæˆæˆ–å‡ºé”™
                    if chunk.get("done", False):
                        break
                    if chunk.get("error"):
                        raise RuntimeError(chunk["error"])
            
            return full_response.strip()
        else:
            # å…¶ä»–æ¨¡å‹å®ç°...
            return "å…¶ä»–æ¨¡å‹æš‚æœªå®ç°"
            
    except Exception as e:
        logger.error(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}")
        return f"[æ‘˜è¦ç”Ÿæˆå¤±è´¥]"
    
    return "[ç©ºæ‘˜è¦]"

@lru_cache(maxsize=100)
def get_text_hash(text: str) -> str:
    """ç”Ÿæˆæ–‡æœ¬æŒ‡çº¹"""
    return hashlib.md5(text.encode()).hexdigest()

@lru_cache(maxsize=100)
def generate_with_retry(prompt: str, level: int, provider: str, max_retries: int = 3) -> str:
    """å¸¦æ¨¡å‹æ¢å¤çš„é‡è¯•æœºåˆ¶"""
    for attempt in range(max_retries):
        try:
            if not check_ollama_service():
                raise RuntimeError("æ¨¡å‹æœåŠ¡ä¸å¯ç”¨")
                
            return generate_summary(prompt, level, provider)
            
        except Exception as e:
            logger.warning(f"å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {str(e)}")
            if attempt == max_retries - 1:
                raise
            time.sleep(5 * (attempt + 1))

def _generate_with_retry_impl(text: str, level: int) -> str:
    """
    æ™ºèƒ½é‡è¯•æœºåˆ¶
    è¿”å›: æˆåŠŸæ—¶è¿”å›æ‘˜è¦ï¼Œå¤±è´¥æ—¶è¿”å›é”™è¯¯ä¿¡æ¯
    """
    provider = "ollama"  # å½“å‰åªå®ç°Ollama
    
    # æ ¹æ®æ–‡æœ¬é•¿åº¦å’Œå±‚çº§è®¡ç®—åŸºç¡€è¶…æ—¶
    base_timeout = min(60, 10 + len(text) / 1000)  # æ¯1000å­—ç¬¦å¢åŠ 1ç§’ï¼Œä¸Šé™60ç§’
    
    for attempt in range(MAX_RETRIES):
        try:
            # åŠ¨æ€è°ƒæ•´è¶…æ—¶(æŒ‡æ•°é€€é¿)
            current_timeout = base_timeout * (attempt + 1)
            
            # è°ƒç”¨å¢å¼ºç‰ˆç”Ÿæˆå‡½æ•°
            result = generate_summary(text, level, provider)
            
            # æ£€æŸ¥ç»“æœæœ‰æ•ˆæ€§
            if "[æ‘˜è¦ç”Ÿæˆå¤±è´¥" in result:
                raise RuntimeError(result)
                
            return result
            
        except requests.exceptions.Timeout:
            logger.warning(f"è¯·æ±‚è¶…æ—¶(å°è¯• {attempt + 1}/{MAX_RETRIES})")
            if attempt == MAX_RETRIES - 1:
                return "[é”™è¯¯] è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€"
                
        except Exception as e:
            logger.error(f"å°è¯• {attempt + 1} å¤±è´¥: {str(e)}")
            if attempt == MAX_RETRIES - 1:
                return f"[é”™è¯¯] æœ€ç»ˆå°è¯•å¤±è´¥: {str(e)}"
        
        # æ™ºèƒ½ç­‰å¾…(æŒ‡æ•°é€€é¿ + æŠ–åŠ¨)
        wait_time = min(30, (2 ** attempt) + random.uniform(0, 1))
        time.sleep(wait_time)
    
    return "[é”™è¯¯] æ‘˜è¦ç”Ÿæˆå¤±è´¥"

def warmup_ollama_model():
    """å¸¦è¿›åº¦æ˜¾ç¤ºçš„æ¨¡å‹é¢„çƒ­"""
    model_name = os.getenv("OLLAMA_MODEL", "phi4-reasoning:plus")
    status = st.empty()
    progress = st.progress(0)
    
    try:
        status.info(f"æ­£åœ¨é¢„çƒ­ {model_name}...")
        
        # åˆ†æ­¥éª¤é¢„çƒ­
        steps = ["åˆå§‹åŒ–", "åŠ è½½æƒé‡", "å‡†å¤‡æ¨ç†"]
        for i, step in enumerate(steps):
            progress.progress(int((i+1)/len(steps)*100))
            status.info(f"{step}...")
            time.sleep(1)  # æ¨¡æ‹Ÿé¢„çƒ­æ­¥éª¤
            
        # å®é™…é¢„çƒ­è¯·æ±‚
        requests.post(
            f"{os.getenv('OLLAMA_API_BASE', 'http://localhost:11434')}/api/show",
            json={"name": model_name},
            timeout=120
        )
        
        progress.progress(100)
        status.success(f"{model_name} é¢„çƒ­å®Œæˆï¼")
    except requests.exceptions.Timeout:
        status.warning("é¢„çƒ­è¶…æ—¶ï¼ˆæœåŠ¡å¯èƒ½å·²å°±ç»ªï¼‰")
    except Exception as e:
        progress.progress(0)
        status.error(f"é¢„çƒ­å¤±è´¥: {str(e)}")
        raise
    finally:
        time.sleep(2)
        progress.empty()
        status.empty()

def preload_ollama_model():
    """å¸¦è¿›åº¦æ¡çš„æ¨¡å‹é¢„åŠ è½½"""
    if not check_ollama_service():
        raise ConnectionError("æ— æ³•è¿æ¥OllamaæœåŠ¡")
    
    model_name = os.getenv("OLLAMA_MODEL", "phi4-reasoning:plus")
    
    # åœ¨é¡µé¢é¡¶éƒ¨åˆ›å»ºçŠ¶æ€å®¹å™¨
    status_container = st.empty()
    progress_bar = st.progress(0)
    status_container.info(f"å¼€å§‹åŠ è½½æ¨¡å‹: {model_name}")
    
    try:
        # åˆ†é˜¶æ®µé¢„åŠ è½½
        stages = ["pull", "load", "warmup"]
        for i, stage in enumerate(stages):
            progress = int((i / len(stages)) * 100)
            progress_bar.progress(progress)
            
            if stage == "pull":
                status_container.info(f"ä¸‹è½½æ¨¡å‹ {model_name}...")
                resp = requests.post(
                    f"{os.getenv('OLLAMA_API_BASE', 'http://localhost:11434')}/api/pull",
                    json={"name": model_name},
                    stream=True
                )
                # å¤„ç†ä¸‹è½½è¿›åº¦
                for _ in resp.iter_lines():
                    progress = min(progress + 2, 33)
                    progress_bar.progress(progress)
                    
            elif stage == "load":
                status_container.info(f"åŠ è½½æ¨¡å‹åˆ°å†…å­˜...")
                resp = requests.post(
                    f"{os.getenv('OLLAMA_API_BASE', 'http://localhost:11434')}/api/generate",
                    json={"model": model_name, "prompt": " "},
                    timeout=120
                )
                progress_bar.progress(66)
                
            else:  # warmup
                status_container.info(f"é¢„çƒ­æ¨¡å‹...")
                warmup_ollama_model()
                progress_bar.progress(100)
                
        status_container.success(f"{model_name} æ¨¡å‹å·²å°±ç»ªï¼")
        
    except Exception as e:
        progress_bar.progress(0)
        status_container.error(f"åŠ è½½å¤±è´¥: {str(e)}")
        raise
    finally:
        time.sleep(2)
        progress_bar.empty()
        status_container.empty()

def check_ollama_service() -> bool:
    """å¢å¼ºç‰ˆæ¨¡å‹æ£€æŸ¥"""
    try:
        # 1. æ£€æŸ¥æœåŠ¡å¯ç”¨æ€§
        resp = requests.get(f"{os.getenv('OLLAMA_API_BASE')}/api/tags", timeout=10)
        if resp.status_code != 200:
            logger.error(f"OllamaæœåŠ¡ä¸å¯ç”¨ (HTTP {resp.status_code})")
            return False
            
        # 2. æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
        active_model = os.getenv('OLLAMA_MODEL', 'phi4-mini-reasoning')
        models = [m['name'] for m in resp.json().get('models', [])]
        
        if active_model not in models:
            logger.error(f"æ¨¡å‹ {active_model} æœªåŠ è½½ï¼Œå¯ç”¨æ¨¡å‹: {', '.join(models)}")
            
            # å°è¯•è‡ªåŠ¨æ‹‰å–æ¨¡å‹
            pull_resp = requests.post(
                f"{os.getenv('OLLAMA_API_BASE')}/api/pull",
                json={"name": active_model},
                timeout=300
            )
            
            if pull_resp.status_code == 200:
                logger.info(f"æ­£åœ¨ä¸‹è½½æ¨¡å‹ {active_model}...")
                return True
            else:
                logger.error(f"æ¨¡å‹ {active_model} ä¸‹è½½å¤±è´¥")
                return False
                
        return True
        
    except Exception as e:
        logger.error(f"æ¨¡å‹æ£€æŸ¥å¤±è´¥: {str(e)}")
        return False

def check_ollama_health() -> bool:
    """æ£€æŸ¥OllamaæœåŠ¡å¥åº·çŠ¶æ€"""
    try:
        start_time = time.time()
        resp = requests.get(
            f"{os.getenv('OLLAMA_API_BASE')}/api/tags",
            timeout=10
        )
        latency = time.time() - start_time
        
        if resp.status_code == 200:
            logger.info(f"OllamaæœåŠ¡æ­£å¸¸ (å“åº”æ—¶é—´: {latency:.2f}s)")
            return latency < 5  # å“åº”æ—¶é—´è¶…è¿‡5ç§’è§†ä¸ºè­¦å‘Š
        else:
            logger.warning(f"OllamaæœåŠ¡å¼‚å¸¸ (HTTP {resp.status_code})")
            return False
    except Exception as e:
        logger.error(f"Ollamaå¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
        return False

def setup_model_management():
    """ä¼˜åŒ–æ¨¡å‹é¢„çƒ­"""
    with st.sidebar.expander("æ¨¡å‹ç®¡ç†", expanded=True):
        if st.button("ğŸ”¥ é¢„çƒ­æ¨¡å‹"):
            try:
                with st.spinner("æ­£åœ¨é¢„çƒ­æ¨¡å‹..."):
                    if model_manager.ensure_model_loaded("ollama"):
                        st.success("âœ… æ¨¡å‹å·²é¢„çƒ­")
                    else:
                        st.error("âŒ é¢„çƒ­å¤±è´¥")
            except Exception as e:
                st.error(f"é¢„çƒ­é”™è¯¯: {str(e)}")
                logger.error(f"æ¨¡å‹é¢„çƒ­å¼‚å¸¸: {str(e)}", exc_info=True)

# åœ¨è¿›ç¨‹å¯åŠ¨æ—¶åˆå§‹åŒ–
model_manager = ModelManager()

# æ·»åŠ LLMç›¸å…³å‡½æ•°
def generate_summary(text: str, level: int, provider: str) -> str:
    """ä¼˜åŒ–ç‰ˆæ‘˜è¦ç”Ÿæˆ(é€Ÿåº¦ä¼˜å…ˆ)"""
    prompt_templates = {
        1: "è¯·ç”¨ä¸­æ–‡ç®€æ´æ€»ç»“ä»¥ä¸‹æ–‡æœ¬çš„å…³é”®å†…å®¹(é™300å­—):\n{text}",
        2: "è¯·ç”¨ä¸­æ–‡æ¦‚æ‹¬ä»¥ä¸‹ç« èŠ‚çš„æ ¸å¿ƒè§‚ç‚¹(é™200å­—):\n{text}",
        3: "è¯·ç”¨3-5å¥ä¸­æ–‡æ€»ç»“å…¨æ–‡ä¸»æ—¨:\n{text}"
    }
    
    # æ›´ä¸¥æ ¼çš„é•¿åº¦é™åˆ¶
    max_input_len = {1: 8000, 2: 5000, 3: 3000}[level]
    text = text[:max_input_len]
    
    try:
        if provider.lower() == "ollama":
            response = requests.post(
                f"{os.getenv('OLLAMA_API_BASE')}/api/generate",
                json={
                    "model": os.getenv("OLLAMA_MODEL"),
                    "prompt": prompt_templates[level].format(text=text),
                    "options": {
                        "temperature": 0.5,  # æé«˜åˆ›é€ æ€§
                        "num_ctx": 2048,     # å‡å°ä¸Šä¸‹æ–‡çª—å£
                        "timeout": 60000,    # 1åˆ†é’Ÿè¶…æ—¶
                        "num_predict": 300   # é™åˆ¶è¾“å‡ºé•¿åº¦
                    },
                    "stream": True
                },
                stream=True,
                timeout=90  # 1.5åˆ†é’Ÿ
            )
            
            # å¤„ç†æµå¼å“åº”
            full_response = ""
            for line in response.iter_lines():
                if line:
                    chunk = json.loads(line)
                    full_response += chunk.get("response", "")
                    
                    # æ£€æŸ¥æ˜¯å¦å®Œæˆæˆ–å‡ºé”™
                    if chunk.get("done", False):
                        break
                    if chunk.get("error"):
                        raise RuntimeError(chunk["error"])
            
            return full_response.strip()
        else:
            # å…¶ä»–æ¨¡å‹å®ç°...
            return "å…¶ä»–æ¨¡å‹æš‚æœªå®ç°"
            
    except Exception as e:
        logger.error(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}")
        return f"[æ‘˜è¦ç”Ÿæˆå¤±è´¥]"
    
    return "[ç©ºæ‘˜è¦]"

@lru_cache(maxsize=100)
def get_text_hash(text: str) -> str:
    """ç”Ÿæˆæ–‡æœ¬æŒ‡çº¹"""
    return hashlib.md5(text.encode()).hexdigest()

@lru_cache(maxsize=100)
def generate_with_retry(prompt: str, level: int, provider: str, max_retries: int = 3) -> str:
    """å¸¦æ¨¡å‹æ¢å¤çš„é‡è¯•æœºåˆ¶"""
    for attempt in range(max_retries):
        try:
            if not check_ollama_service():
                raise RuntimeError("æ¨¡å‹æœåŠ¡ä¸å¯ç”¨")
                
            return generate_summary(prompt, level, provider)
            
        except Exception as e:
            logger.warning(f"å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {str(e)}")
            if attempt == max_retries - 1:
                raise
            time.sleep(5 * (attempt + 1))

def process_chunk_parallel(chunk: str, chunk_id: int, provider: str) -> tuple[int, str]:
    """å¼ºåˆ¶ä¸­æ–‡è¾“å‡ºçš„åˆ†å—å¤„ç†ï¼Œè‹±æ–‡è‡ªåŠ¨é‡è¯•"""
    for attempt in range(3):
        try:
            model_manager.ensure_model_loaded(provider)
            prompt = (
                "ä½ æ˜¯ä¸€åä¸­æ–‡ä¸“ä¸šæ–‡çŒ®æ€»ç»“åŠ©æ‰‹ã€‚"
                "è¯·ç”¨ç®€ä½“ä¸­æ–‡ã€ç»“æ„åŒ–ã€æ¡ç†æ¸…æ™°åœ°æ€»ç»“ä»¥ä¸‹å†…å®¹ï¼ˆé™300å­—ï¼‰ï¼š\n\n"
                f"{chunk}"
            )
            response = requests.post(
                f"{os.getenv('OLLAMA_API_BASE')}/api/generate",
                json={
                    "model": os.getenv("OLLAMA_MODEL"),
                    "prompt": prompt,
                    "options": {
                        "temperature": 0.2,
                        "num_ctx": 2048
                    }
                },
                timeout=OLLAMA_TIMEOUT
            )
            result = ""
            for line in response.iter_lines():
                if line:
                    data = json.loads(line)
                    result += data.get("response", "")
                    if data.get("done", False):
                        break
            # æ£€æŸ¥æ˜¯å¦ä¸ºè‹±æ–‡ï¼Œè‹¥æ˜¯åˆ™æŠ¥é”™æˆ–é‡è¯•
            if re.search(r'[a-zA-Z]{8,}', result) and not re.search(r'[\u4e00-\u9fa5]', result):
                raise ValueError("æ¨¡å‹è¿”å›è‹±æ–‡æ‘˜è¦ï¼Œé‡è¯•")
            return (chunk_id, result.strip() or "[æ— æœ‰æ•ˆæ‘˜è¦]")
        except Exception as e:
            logger.warning(f"åˆ†å— {chunk_id} å¤„ç†å¤±è´¥: {str(e)}")
            if attempt == 2:
                return (chunk_id, f"[å¤„ç†å¤±è´¥: {str(e)}]")
            time.sleep(OLLAMA_RETRY_DELAY * (attempt+1))

def get_optimal_chunk_size(text: str) -> int:
    """åŠ¨æ€è®¡ç®—æœ€ä½³åˆ†å—å¤§å°"""
    # æŠ€æœ¯æ–‡æ¡£ç‰¹å¾æ£€æµ‹
    if any(keyword in text[:1000] for keyword in 
           ['Abstract', 'å¼•è¨€', 'å®éªŒæ–¹æ³•', 'å‚è€ƒæ–‡çŒ®']):
        return 4000  # æŠ€æœ¯æ–‡æ¡£ç”¨å°åˆ†å—
    
    # å°è¯´/æ•£æ–‡æ£€æµ‹
    if any(keyword in text[:1000] for keyword in 
           ['ç¬¬[ä¸€äºŒä¸‰å››]ç« ', 'CHAPTER', '......']):
        return 8000  # æ–‡å­¦ä½œå“ç”¨å¤§åˆ†å—
        
    return 6000  # é»˜è®¤å€¼

def split_into_chunks(text: str) -> list[str]:
    """æ™ºèƒ½åˆ†å—å‡½æ•°"""
    CHUNK_SIZE = get_optimal_chunk_size(text)
    
    # æŒ‰æ®µè½åˆ†å—(ä¿æŒè¯­ä¹‰å®Œæ•´)
    paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
    chunks = []
    current_chunk = ""
    
    for para in paragraphs:
        if len(current_chunk) + len(para) > CHUNK_SIZE and current_chunk:
            chunks.append(current_chunk)
            current_chunk = para
        else:
            current_chunk += "\n" + para
    
    if current_chunk:
        chunks.append(current_chunk)
    
    logger.info(f"å°†æ–‡æœ¬åˆ†æˆ {len(chunks)} ä¸ªå—ï¼Œå¹³å‡å¤§å°: {sum(len(c) for c in chunks)//len(chunks)}å­—ç¬¦")
    return chunks

def generate_hierarchical_summary(text: str, provider: str) -> dict:
    """å®Œæ•´ä¿®å¤ç‰ˆåˆ†å±‚æ‘˜è¦"""
    result = {
        'chunks': [],       # åŸå§‹åˆ†å—
        'summaries': [],    # åˆ†å—æ‘˜è¦
        'sections': [],     # å¯ä¸‹é’»ç« èŠ‚
        'final_summary': ""
    }
    
    try:
        # 1. åˆ†å—å¤„ç†
        chunks = split_into_chunks(text)
        result['chunks'] = chunks
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = [executor.submit(process_chunk_parallel, chunk, i, provider) 
                      for i, chunk in enumerate(chunks)]
            
            for future in concurrent.futures.as_completed(futures):
                chunk_id, summary = future.result()
                result['summaries'].append((chunk_id, summary))
        
        # 2. æ„å»ºå¯ä¸‹é’»ç« èŠ‚
        result['summaries'].sort(key=lambda x: x[0])
        summaries = [s for _, s in result['summaries'] if not s.startswith('[')]
        
        SECTION_SIZE = 3  # æ¯èŠ‚åŒ…å«çš„åˆ†å—æ•°
        for section_idx in range(0, len(summaries), SECTION_SIZE):
            section_chunks = summaries[section_idx:section_idx+SECTION_SIZE]
            section_content = "\n\n".join([
                f"### åˆ†å— {section_idx+i+1}\n{chunk}" 
                for i, chunk in enumerate(section_chunks)
            ])
            
            result['sections'].append({
                "title": f"ç¬¬{section_idx//SECTION_SIZE +1}èŠ‚",
                "content": section_content,
                "chunk_ids": list(range(section_idx, section_idx+len(section_chunks)))
            })
        
        # 3. ç”Ÿæˆæœ€ç»ˆæ‘˜è¦
        if result['sections']:
            section_texts = [f"{s['title']}: {s['content'][:200]}..." for s in result['sections']]
            result['final_summary'] = generate_final_summary(section_texts)
            
    except Exception as e:
        logger.error(f"å¤„ç†å¤±è´¥: {str(e)}", exc_info=True)
        result['error'] = str(e)
    
    return result

def check_pause(pause_btn: bool, resume_btn: bool, placeholder):
    """æ£€æŸ¥å¹¶å¤„ç†æš‚åœçŠ¶æ€"""
    if pause_btn:
        with placeholder.container():
            st.session_state.paused = True
            st.button("â–¶ï¸ ç»§ç»­", key="resume_btn")
        
        while st.session_state.paused:
            time.sleep(0.5)
            if resume_btn:
                st.session_state.paused = False
                break

def preprocess_text(text: str) -> str:
    """ç»ˆææ–‡æœ¬é¢„å¤„ç†"""
    # ç§»é™¤PDFå¸¸è§å™ªå£°
    patterns = [
        r'\d{1,3}\s+[\u4e00-\u9fa5]+\s+\d{1,3}',  # é¡µçœ‰é¡µç 
        r'Â©.*\d{4}',  # ç‰ˆæƒä¿¡æ¯
        r'http[s]?://\S+',  # URLé“¾æ¥
        r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'  # é‚®ç®±
    ]
    
    for pattern in patterns:
        text = re.sub(pattern, '', text)
    
    # æ™ºèƒ½æ®µè½è¿‡æ»¤
    lines = []
    for line in text.split('\n'):
        line = line.strip()
        if not line:
            continue
            
        # ä¿ç•™æœ‰æ•ˆæ®µè½
        if len(line) > 25 or \
           any(c.isalpha() for c in line) or \
           line.endswith(('ã€‚', '!', '?', ';')):
            lines.append(line)
    
    return '\n'.join(lines)

def generate_final_summary(section_summaries: list[str]) -> str:
    """ç”Ÿæˆæœ€ç»ˆæ‘˜è¦"""
    if not section_summaries:
        return "[æ— æœ‰æ•ˆæ‘˜è¦å†…å®¹]"
    
    # åˆå¹¶æ‰€æœ‰ç« èŠ‚æ‘˜è¦
    combined = "\n\n".join(section_summaries)
    
    try:
        # è°ƒç”¨Ollamaç”Ÿæˆæœ€ç»ˆæ‘˜è¦
        response = requests.post(
            f"{os.getenv('OLLAMA_API_BASE')}/api/generate",
            json={
                "model": os.getenv("OLLAMA_MODEL"),
                "prompt": f"è¯·æ ¹æ®ä»¥ä¸‹ç« èŠ‚æ‘˜è¦ï¼Œç”¨ä¸­æ–‡ç”Ÿæˆä¸€ä¸ªç»“æ„åŒ–çš„æœ€ç»ˆæ€»ç»“(500å­—ä»¥å†…):\n\n{combined}",
                "options": {"temperature": 0.3, "num_ctx": 4096}
            },
            timeout=120
        )
        
        if response.status_code == 200:
            result = ""
            for line in response.iter_lines():
                if line:
                    data = json.loads(line)
                    result += data.get("response", "")
                    if data.get("done", False):
                        break
            return result.strip()
        else:
            return "[æ‘˜è¦ç”Ÿæˆå¤±è´¥]"
    except Exception as e:
        logger.error(f"æœ€ç»ˆæ‘˜è¦ç”Ÿæˆé”™è¯¯: {str(e)}")
        return "[æ‘˜è¦ç”Ÿæˆå¼‚å¸¸]"

def display_drilldown_summary(result):
    # åˆå§‹åŒ–çŠ¶æ€
    if "current_level" not in st.session_state:
        st.session_state.current_level = 1
    if "selected_section" not in st.session_state:
        st.session_state.selected_section = 0
    if "selected_chunk" not in st.session_state:
        st.session_state.selected_chunk = 0

    # 1çº§ï¼šé¦–é¡µ/æ€»è§ˆ
    if st.session_state.current_level == 1:
        st.markdown("## æœ€ç»ˆæ‘˜è¦")
        st.markdown(result['final_summary'])
        st.markdown("## ç« èŠ‚æ‘˜è¦")
        for idx, section in enumerate(result['sections']):
            st.markdown(f"**{section['title']}**\n\n{section['content']}")
            if st.button(f"ä¸‹é’»åˆ°{section['title']}", key=f"drill_section_{idx}"):
                st.session_state.selected_section = idx
                st.session_state.current_level = 2
                st.rerun()

    # 2çº§ï¼šç« èŠ‚ä¸‹åˆ†å—æ‘˜è¦
    elif st.session_state.current_level == 2:
        section = result['sections'][st.session_state.selected_section]
        st.markdown(f"### {section['title']} æ‘˜è¦")
        st.markdown(section['content'])
        st.markdown("---")
        st.markdown("#### åˆ†å—æ‘˜è¦")
        for i, idx in enumerate(section['chunk_ids']):
            chunk_summary = next((s for j, s in result['summaries'] if j == idx), None)
            if chunk_summary:
                st.markdown(f"**åˆ†å— {idx+1}**\n{chunk_summary}")
                if st.button(f"ä¸‹é’»åˆ°åˆ†å— {idx+1}", key=f"drill_chunk_{idx}"):
                    st.session_state.selected_chunk = idx
                    st.session_state.current_level = 3
                    st.rerun()
        if st.button("è¿”å›ç« èŠ‚æ€»è§ˆ"):
            st.session_state.current_level = 1
            st.rerun()

    # 3çº§ï¼šåˆ†å—åŸæ–‡
    elif st.session_state.current_level == 3:
        chunk_id = st.session_state.selected_chunk
        st.markdown(f"### åˆ†å— {chunk_id+1} åŸæ–‡")
        st.text(result['chunks'][chunk_id][:2000])
        if st.button("è¿”å›ä¸Šä¸€å±‚ï¼ˆç« èŠ‚ï¼‰"):
            st.session_state.current_level = 2
            st.rerun()

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'summaries' not in st.session_state:
    st.session_state.summaries = {}
    
if 'full_text' not in st.session_state:
    st.session_state.full_text = ""
    
if 'current_level' not in st.session_state:
    st.session_state.current_level = 1

# ä¸»ç•Œé¢
st.title("PDFå¿«é€Ÿé˜…è¯»åŠ©æ‰‹")
st.subheader("å±‚æ¬¡åŒ–æ‘˜è¦å·¥å…·")

# ä¾§è¾¹æ è®¾ç½®
with st.sidebar:
    model_provider = st.selectbox(
        "é€‰æ‹©æ‘˜è¦æ¨¡å‹",
        ["OpenAI", "DeepSeek", "Ollama"],
        key="sidebar_model_provider"
    )
    
    setup_model_management()

# PDFä¸Šä¼ 
uploaded_file = st.file_uploader("ä¸Šä¼ PDFæ–‡ä»¶", type="pdf")

if uploaded_file:
    try:
        # ä½¿ç”¨å•ä¸€çŠ¶æ€å®¹å™¨
        status_container = st.empty()
        progress_bar = st.progress(0)
        
        status_container.info("å¼€å§‹å¤„ç†PDF...")
        progress_bar.progress(10)
        
        # 1. è§£æPDF
        pdf_data = process_pdf(uploaded_file)
        progress_bar.progress(30)
        
        # 2. åˆ†å±‚æ‘˜è¦
        status_container.info("å¼€å§‹åˆ†å±‚æ‘˜è¦...")
        result = generate_hierarchical_summary(
            text=pdf_data['text'],
            provider=model_provider
        )
        progress_bar.progress(90)
        
        # ä¿å­˜ç»“æœ
        st.session_state["full_text"] = pdf_data["text"]
        st.session_state["chunk_summaries"] = result["summaries"]
        st.session_state["section_summaries"] = result["sections"]
        st.session_state["final_summary"] = result["final_summary"]
        
        progress_bar.progress(100)
        status_container.success("å¤„ç†å®Œæˆï¼")
        
        # æ˜¾ç¤ºç»“æœå¯¼èˆª
        display_drilldown_summary(result)
        
    except Exception as e:
        st.error(f"å¤„ç†å¤±è´¥: {str(e)}")
        logger.error(f"å¤„ç†é”™è¯¯: {str(e)}", exc_info=True)
    finally:
        time.sleep(2)
        progress_bar.empty()
        status_container.empty()

# å¯¼èˆªæ§åˆ¶
col1, col2 = st.columns(2)
with col1:
    if st.button("â† ä¸Šä¸€å±‚çº§") and st.session_state.current_level > 1:
        st.session_state.current_level -= 1
        st.rerun()
with col2:
    if st.button("é‡ç½®åˆ°ç¬¬ä¸€å±‚"):
        st.session_state.current_level = 1
        st.rerun()
