import streamlit as st
import json
import logging
import time
import os
from PyPDF2 import PdfReader
import io
from io import BytesIO
import requests
import random
import concurrent.futures
from functools import partial, lru_cache
import re
import hashlib
import threading

# ç¡®ä¿UTF-8æ—¥å¿—å¤„ç†
class UTF8StreamHandler(logging.StreamHandler):
    def emit(self, record):
        try:
            msg = self.format(record)
            if not isinstance(msg, str):
                msg = str(msg)
            stream = self.stream
            stream.write(msg.encode('utf-8').decode('utf-8') + self.terminator)
            self.flush()
        except Exception:
            self.handleError(record)

# Ensure logs directory exists
os.makedirs("logs", exist_ok=True)

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/app.log', encoding='utf-8'),
        UTF8StreamHandler()  # æ›¿æ¢åŸæ¥çš„StreamHandler
    ]
)
logger = logging.getLogger(__name__)

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ é…ç½®
MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
REQUEST_TIMEOUT = 120  # 120ç§’
MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
CHUNK_SIZE = 1024 * 1024  # 1MB
MAX_PAGES_PER_REQUEST = 10  # æ¯æ¬¡è¯·æ±‚å¤„ç†çš„æœ€å¤§é¡µæ•°
MAX_WORKERS = 8  # æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´

# å…¨å±€æ¨¡å‹ç®¡ç†å™¨
class ModelManager:
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._model_loaded = False
            cls._instance._lock = threading.Lock()
        return cls._instance
    
    def ensure_model_loaded(self, provider: str):
        """ç¡®ä¿æ¨¡å‹å·²åŠ è½½(çº¿ç¨‹å®‰å…¨)"""
        if self._model_loaded:
            return True
            
        with self._lock:
            if not self._model_loaded:  # åŒé‡æ£€æŸ¥é”å®š
                if provider == "ollama" and not self._load_ollama_model():
                    raise RuntimeError("æ¨¡å‹åŠ è½½å¤±è´¥")
                self._model_loaded = True
        return True
    
    def _load_ollama_model(self) -> bool:
        """å®é™…åŠ è½½Ollamaæ¨¡å‹"""
        try:
            model_name = os.getenv('OLLAMA_MODEL')
            resp = requests.post(
                f"{os.getenv('OLLAMA_API_BASE')}/api/generate",
                json={"model": model_name, "prompt": ""},
                timeout=60
            )
            return resp.status_code == 200
        except Exception as e:
            logger.error(f"æ¨¡å‹é¢„çƒ­å¤±è´¥: {str(e)}")
            return False

# åœ¨è¿›ç¨‹å¯åŠ¨æ—¶åˆå§‹åŒ–
model_manager = ModelManager()

# æ·»åŠ LLMç›¸å…³å‡½æ•°
def generate_summary(text: str, level: int, provider: str) -> str:
    """ä¼˜åŒ–ç‰ˆæ‘˜è¦ç”Ÿæˆ(é€Ÿåº¦ä¼˜å…ˆ)"""
    prompt_templates = {
        1: "è¯·ç”¨ä¸­æ–‡ç®€æ´æ€»ç»“ä»¥ä¸‹æ–‡æœ¬çš„å…³é”®å†…å®¹(é™300å­—):\n{text}",
        2: "è¯·ç”¨ä¸­æ–‡æ¦‚æ‹¬ä»¥ä¸‹ç« èŠ‚çš„æ ¸å¿ƒè§‚ç‚¹(é™200å­—):\n{text}",
        3: "è¯·ç”¨3-5å¥ä¸­æ–‡æ€»ç»“å…¨æ–‡ä¸»æ—¨:\n{text}"
    }
    
    # æ›´ä¸¥æ ¼çš„é•¿åº¦é™åˆ¶
    max_input_len = {1: 8000, 2: 5000, 3: 3000}[level]
    text = text[:max_input_len]
    
    try:
        if provider.lower() == "ollama":
            response = requests.post(
                f"{os.getenv('OLLAMA_API_BASE')}/api/generate",
                json={
                    "model": os.getenv("OLLAMA_MODEL"),
                    "prompt": prompt_templates[level].format(text=text),
                    "options": {
                        "temperature": 0.5,  # æé«˜åˆ›é€ æ€§
                        "num_ctx": 2048,     # å‡å°ä¸Šä¸‹æ–‡çª—å£
                        "timeout": 60000,    # 1åˆ†é’Ÿè¶…æ—¶
                        "num_predict": 300   # é™åˆ¶è¾“å‡ºé•¿åº¦
                    },
                    "stream": True
                },
                stream=True,
                timeout=90  # 1.5åˆ†é’Ÿ
            )
            
            # å¤„ç†æµå¼å“åº”
            full_response = ""
            for line in response.iter_lines():
                if line:
                    chunk = json.loads(line)
                    full_response += chunk.get("response", "")
                    
                    # æ£€æŸ¥æ˜¯å¦å®Œæˆæˆ–å‡ºé”™
                    if chunk.get("done", False):
                        break
                    if chunk.get("error"):
                        raise RuntimeError(chunk["error"])
            
            return full_response.strip()
        else:
            # å…¶ä»–æ¨¡å‹å®ç°...
            return "å…¶ä»–æ¨¡å‹æš‚æœªå®ç°"
            
    except Exception as e:
        logger.error(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}")
        return f"[æ‘˜è¦ç”Ÿæˆå¤±è´¥]"
    
    return "[ç©ºæ‘˜è¦]"

@lru_cache(maxsize=100)
def get_text_hash(text: str) -> str:
    """ç”Ÿæˆæ–‡æœ¬æŒ‡çº¹"""
    return hashlib.md5(text.encode()).hexdigest()

@lru_cache(maxsize=100)
def generate_with_retry(prompt: str, level: int, provider: str, max_retries: int = 3) -> str:
    """å¸¦æ¨¡å‹æ¢å¤çš„é‡è¯•æœºåˆ¶"""
    for attempt in range(max_retries):
        try:
            if not check_ollama_service():
                raise RuntimeError("æ¨¡å‹æœåŠ¡ä¸å¯ç”¨")
                
            return generate_summary(prompt, level, provider)
            
        except Exception as e:
            logger.warning(f"å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {str(e)}")
            if attempt == max_retries - 1:
                raise
            time.sleep(5 * (attempt + 1))

def _generate_with_retry_impl(text: str, level: int) -> str:
    """
    æ™ºèƒ½é‡è¯•æœºåˆ¶
    è¿”å›: æˆåŠŸæ—¶è¿”å›æ‘˜è¦ï¼Œå¤±è´¥æ—¶è¿”å›é”™è¯¯ä¿¡æ¯
    """
    provider = "ollama"  # å½“å‰åªå®ç°Ollama
    
    # æ ¹æ®æ–‡æœ¬é•¿åº¦å’Œå±‚çº§è®¡ç®—åŸºç¡€è¶…æ—¶
    base_timeout = min(60, 10 + len(text) / 1000)  # æ¯1000å­—ç¬¦å¢åŠ 1ç§’ï¼Œä¸Šé™60ç§’
    
    for attempt in range(MAX_RETRIES):
        try:
            # åŠ¨æ€è°ƒæ•´è¶…æ—¶(æŒ‡æ•°é€€é¿)
            current_timeout = base_timeout * (attempt + 1)
            
            # è°ƒç”¨å¢å¼ºç‰ˆç”Ÿæˆå‡½æ•°
            result = generate_summary(text, level, provider)
            
            # æ£€æŸ¥ç»“æœæœ‰æ•ˆæ€§
            if "[æ‘˜è¦ç”Ÿæˆå¤±è´¥" in result:
                raise RuntimeError(result)
                
            return result
            
        except requests.exceptions.Timeout:
            logger.warning(f"è¯·æ±‚è¶…æ—¶(å°è¯• {attempt + 1}/{MAX_RETRIES})")
            if attempt == MAX_RETRIES - 1:
                return "[é”™è¯¯] è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€"
                
        except Exception as e:
            logger.error(f"å°è¯• {attempt + 1} å¤±è´¥: {str(e)}")
            if attempt == MAX_RETRIES - 1:
                return f"[é”™è¯¯] æœ€ç»ˆå°è¯•å¤±è´¥: {str(e)}"
        
        # æ™ºèƒ½ç­‰å¾…(æŒ‡æ•°é€€é¿ + æŠ–åŠ¨)
        wait_time = min(30, (2 ** attempt) + random.uniform(0, 1))
        time.sleep(wait_time)
    
    return "[é”™è¯¯] æ‘˜è¦ç”Ÿæˆå¤±è´¥"

def warmup_ollama_model():
    """å¸¦è¿›åº¦æ˜¾ç¤ºçš„æ¨¡å‹é¢„çƒ­"""
    model_name = os.getenv("OLLAMA_MODEL", "phi4-reasoning:plus")
    status = st.empty()
    progress = st.progress(0)
    
    try:
        status.info(f"æ­£åœ¨é¢„çƒ­ {model_name}...")
        
        # åˆ†æ­¥éª¤é¢„çƒ­
        steps = ["åˆå§‹åŒ–", "åŠ è½½æƒé‡", "å‡†å¤‡æ¨ç†"]
        for i, step in enumerate(steps):
            progress.progress(int((i+1)/len(steps)*100))
            status.info(f"{step}...")
            time.sleep(1)  # æ¨¡æ‹Ÿé¢„çƒ­æ­¥éª¤
            
        # å®é™…é¢„çƒ­è¯·æ±‚
        requests.post(
            f"{os.getenv('OLLAMA_API_BASE', 'http://localhost:11434')}/api/show",
            json={"name": model_name},
            timeout=120
        )
        
        progress.progress(100)
        status.success(f"{model_name} é¢„çƒ­å®Œæˆï¼")
    except requests.exceptions.Timeout:
        status.warning("é¢„çƒ­è¶…æ—¶ï¼ˆæœåŠ¡å¯èƒ½å·²å°±ç»ªï¼‰")
    except Exception as e:
        progress.progress(0)
        status.error(f"é¢„çƒ­å¤±è´¥: {str(e)}")
        raise
    finally:
        time.sleep(2)
        progress.empty()
        status.empty()

def preload_ollama_model():
    """å¸¦è¿›åº¦æ¡çš„æ¨¡å‹é¢„åŠ è½½"""
    if not check_ollama_service():
        raise ConnectionError("æ— æ³•è¿æ¥OllamaæœåŠ¡")
    
    model_name = os.getenv("OLLAMA_MODEL", "phi4-reasoning:plus")
    
    # åœ¨é¡µé¢é¡¶éƒ¨åˆ›å»ºçŠ¶æ€å®¹å™¨
    status_container = st.empty()
    progress_bar = st.progress(0)
    status_container.info(f"å¼€å§‹åŠ è½½æ¨¡å‹: {model_name}")
    
    try:
        # åˆ†é˜¶æ®µé¢„åŠ è½½
        stages = ["pull", "load", "warmup"]
        for i, stage in enumerate(stages):
            progress = int((i / len(stages)) * 100)
            progress_bar.progress(progress)
            
            if stage == "pull":
                status_container.info(f"ä¸‹è½½æ¨¡å‹ {model_name}...")
                resp = requests.post(
                    f"{os.getenv('OLLAMA_API_BASE', 'http://localhost:11434')}/api/pull",
                    json={"name": model_name},
                    stream=True
                )
                # å¤„ç†ä¸‹è½½è¿›åº¦
                for _ in resp.iter_lines():
                    progress = min(progress + 2, 33)
                    progress_bar.progress(progress)
                    
            elif stage == "load":
                status_container.info(f"åŠ è½½æ¨¡å‹åˆ°å†…å­˜...")
                resp = requests.post(
                    f"{os.getenv('OLLAMA_API_BASE', 'http://localhost:11434')}/api/generate",
                    json={"model": model_name, "prompt": " "},
                    timeout=120
                )
                progress_bar.progress(66)
                
            else:  # warmup
                status_container.info(f"é¢„çƒ­æ¨¡å‹...")
                warmup_ollama_model()
                progress_bar.progress(100)
                
        status_container.success(f"{model_name} æ¨¡å‹å·²å°±ç»ªï¼")
        
    except Exception as e:
        progress_bar.progress(0)
        status_container.error(f"åŠ è½½å¤±è´¥: {str(e)}")
        raise
    finally:
        time.sleep(2)
        progress_bar.empty()
        status_container.empty()

def check_ollama_service() -> bool:
    """å¢å¼ºç‰ˆæ¨¡å‹æ£€æŸ¥"""
    try:
        # 1. æ£€æŸ¥æœåŠ¡å¯ç”¨æ€§
        resp = requests.get(f"{os.getenv('OLLAMA_API_BASE')}/api/tags", timeout=10)
        if resp.status_code != 200:
            logger.error(f"OllamaæœåŠ¡ä¸å¯ç”¨ (HTTP {resp.status_code})")
            return False
            
        # 2. æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
        active_model = os.getenv('OLLAMA_MODEL', 'phi4-mini-reasoning')
        models = [m['name'] for m in resp.json().get('models', [])]
        
        if active_model not in models:
            logger.error(f"æ¨¡å‹ {active_model} æœªåŠ è½½ï¼Œå¯ç”¨æ¨¡å‹: {', '.join(models)}")
            
            # å°è¯•è‡ªåŠ¨æ‹‰å–æ¨¡å‹
            pull_resp = requests.post(
                f"{os.getenv('OLLAMA_API_BASE')}/api/pull",
                json={"name": active_model},
                timeout=300
            )
            
            if pull_resp.status_code == 200:
                logger.info(f"æ­£åœ¨ä¸‹è½½æ¨¡å‹ {active_model}...")
                return True
            else:
                logger.error(f"æ¨¡å‹ {active_model} ä¸‹è½½å¤±è´¥")
                return False
                
        return True
        
    except Exception as e:
        logger.error(f"æ¨¡å‹æ£€æŸ¥å¤±è´¥: {str(e)}")
        return False

def setup_model_management():
    """ä¼˜åŒ–æ¨¡å‹é¢„çƒ­"""
    with st.sidebar.expander("æ¨¡å‹ç®¡ç†", expanded=True):
        if st.button("ğŸ”¥ é¢„çƒ­æ¨¡å‹"):
            try:
                with st.spinner("æ­£åœ¨é¢„çƒ­æ¨¡å‹..."):
                    if model_manager.ensure_model_loaded("ollama"):
                        st.success("âœ… æ¨¡å‹å·²é¢„çƒ­")
                    else:
                        st.error("âŒ é¢„çƒ­å¤±è´¥")
            except Exception as e:
                st.error(f"é¢„çƒ­é”™è¯¯: {str(e)}")
                logger.error(f"æ¨¡å‹é¢„çƒ­å¼‚å¸¸: {str(e)}", exc_info=True)

# åœ¨è¿›ç¨‹å¯åŠ¨æ—¶åˆå§‹åŒ–
model_manager = ModelManager()

# æ·»åŠ LLMç›¸å…³å‡½æ•°
def generate_summary(text: str, level: int, provider: str) -> str:
    """ä¼˜åŒ–ç‰ˆæ‘˜è¦ç”Ÿæˆ(é€Ÿåº¦ä¼˜å…ˆ)"""
    prompt_templates = {
        1: "è¯·ç”¨ä¸­æ–‡ç®€æ´æ€»ç»“ä»¥ä¸‹æ–‡æœ¬çš„å…³é”®å†…å®¹(é™300å­—):\n{text}",
        2: "è¯·ç”¨ä¸­æ–‡æ¦‚æ‹¬ä»¥ä¸‹ç« èŠ‚çš„æ ¸å¿ƒè§‚ç‚¹(é™200å­—):\n{text}",
        3: "è¯·ç”¨3-5å¥ä¸­æ–‡æ€»ç»“å…¨æ–‡ä¸»æ—¨:\n{text}"
    }
    
    # æ›´ä¸¥æ ¼çš„é•¿åº¦é™åˆ¶
    max_input_len = {1: 8000, 2: 5000, 3: 3000}[level]
    text = text[:max_input_len]
    
    try:
        if provider.lower() == "ollama":
            response = requests.post(
                f"{os.getenv('OLLAMA_API_BASE')}/api/generate",
                json={
                    "model": os.getenv("OLLAMA_MODEL"),
                    "prompt": prompt_templates[level].format(text=text),
                    "options": {
                        "temperature": 0.5,  # æé«˜åˆ›é€ æ€§
                        "num_ctx": 2048,     # å‡å°ä¸Šä¸‹æ–‡çª—å£
                        "timeout": 60000,    # 1åˆ†é’Ÿè¶…æ—¶
                        "num_predict": 300   # é™åˆ¶è¾“å‡ºé•¿åº¦
                    },
                    "stream": True
                },
                stream=True,
                timeout=90  # 1.5åˆ†é’Ÿ
            )
            
            # å¤„ç†æµå¼å“åº”
            full_response = ""
            for line in response.iter_lines():
                if line:
                    chunk = json.loads(line)
                    full_response += chunk.get("response", "")
                    
                    # æ£€æŸ¥æ˜¯å¦å®Œæˆæˆ–å‡ºé”™
                    if chunk.get("done", False):
                        break
                    if chunk.get("error"):
                        raise RuntimeError(chunk["error"])
            
            return full_response.strip()
        else:
            # å…¶ä»–æ¨¡å‹å®ç°...
            return "å…¶ä»–æ¨¡å‹æš‚æœªå®ç°"
            
    except Exception as e:
        logger.error(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}")
        return f"[æ‘˜è¦ç”Ÿæˆå¤±è´¥]"
    
    return "[ç©ºæ‘˜è¦]"

@lru_cache(maxsize=100)
def get_text_hash(text: str) -> str:
    """ç”Ÿæˆæ–‡æœ¬æŒ‡çº¹"""
    return hashlib.md5(text.encode()).hexdigest()

@lru_cache(maxsize=100)
def generate_with_retry(prompt: str, level: int, provider: str, max_retries: int = 3) -> str:
    """å¸¦æ¨¡å‹æ¢å¤çš„é‡è¯•æœºåˆ¶"""
    for attempt in range(max_retries):
        try:
            if not check_ollama_service():
                raise RuntimeError("æ¨¡å‹æœåŠ¡ä¸å¯ç”¨")
                
            return generate_summary(prompt, level, provider)
            
        except Exception as e:
            logger.warning(f"å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {str(e)}")
            if attempt == max_retries - 1:
                raise
            time.sleep(5 * (attempt + 1))

def _generate_with_retry_impl(text: str, level: int) -> str:
    """
    æ™ºèƒ½é‡è¯•æœºåˆ¶
    è¿”å›: æˆåŠŸæ—¶è¿”å›æ‘˜è¦ï¼Œå¤±è´¥æ—¶è¿”å›é”™è¯¯ä¿¡æ¯
    """
    provider = "ollama"  # å½“å‰åªå®ç°Ollama
    
    # æ ¹æ®æ–‡æœ¬é•¿åº¦å’Œå±‚çº§è®¡ç®—åŸºç¡€è¶…æ—¶
    base_timeout = min(60, 10 + len(text) / 1000)  # æ¯1000å­—ç¬¦å¢åŠ 1ç§’ï¼Œä¸Šé™60ç§’
    
    for attempt in range(MAX_RETRIES):
        try:
            # åŠ¨æ€è°ƒæ•´è¶…æ—¶(æŒ‡æ•°é€€é¿)
            current_timeout = base_timeout * (attempt + 1)
            
            # è°ƒç”¨å¢å¼ºç‰ˆç”Ÿæˆå‡½æ•°
            result = generate_summary(text, level, provider)
            
            # æ£€æŸ¥ç»“æœæœ‰æ•ˆæ€§
            if "[æ‘˜è¦ç”Ÿæˆå¤±è´¥" in result:
                raise RuntimeError(result)
                
            return result
            
        except requests.exceptions.Timeout:
            logger.warning(f"è¯·æ±‚è¶…æ—¶(å°è¯• {attempt + 1}/{MAX_RETRIES})")
            if attempt == MAX_RETRIES - 1:
                return "[é”™è¯¯] è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€"
                
        except Exception as e:
            logger.error(f"å°è¯• {attempt + 1} å¤±è´¥: {str(e)}")
            if attempt == MAX_RETRIES - 1:
                return f"[é”™è¯¯] æœ€ç»ˆå°è¯•å¤±è´¥: {str(e)}"
        
        # æ™ºèƒ½ç­‰å¾…(æŒ‡æ•°é€€é¿ + æŠ–åŠ¨)
        wait_time = min(30, (2 ** attempt) + random.uniform(0, 1))
        time.sleep(wait_time)
    
    return "[é”™è¯¯] æ‘˜è¦ç”Ÿæˆå¤±è´¥"

def warmup_ollama_model():
    """å¸¦è¿›åº¦æ˜¾ç¤ºçš„æ¨¡å‹é¢„çƒ­"""
    model_name = os.getenv("OLLAMA_MODEL", "phi4-reasoning:plus")
    status = st.empty()
    progress = st.progress(0)
    
    try:
        status.info(f"æ­£åœ¨é¢„çƒ­ {model_name}...")
        
        # åˆ†æ­¥éª¤é¢„çƒ­
        steps = ["åˆå§‹åŒ–", "åŠ è½½æƒé‡", "å‡†å¤‡æ¨ç†"]
        for i, step in enumerate(steps):
            progress.progress(int((i+1)/len(steps)*100))
            status.info(f"{step}...")
            time.sleep(1)  # æ¨¡æ‹Ÿé¢„çƒ­æ­¥éª¤
            
        # å®é™…é¢„çƒ­è¯·æ±‚
        requests.post(
            f"{os.getenv('OLLAMA_API_BASE', 'http://localhost:11434')}/api/show",
            json={"name": model_name},
            timeout=120
        )
        
        progress.progress(100)
        status.success(f"{model_name} é¢„çƒ­å®Œæˆï¼")
    except requests.exceptions.Timeout:
        status.warning("é¢„çƒ­è¶…æ—¶ï¼ˆæœåŠ¡å¯èƒ½å·²å°±ç»ªï¼‰")
    except Exception as e:
        progress.progress(0)
        status.error(f"é¢„çƒ­å¤±è´¥: {str(e)}")
        raise
    finally:
        time.sleep(2)
        progress.empty()
        status.empty()

def preload_ollama_model():
    """å¸¦è¿›åº¦æ¡çš„æ¨¡å‹é¢„åŠ è½½"""
    if not check_ollama_service():
        raise ConnectionError("æ— æ³•è¿æ¥OllamaæœåŠ¡")
    
    model_name = os.getenv("OLLAMA_MODEL", "phi4-reasoning:plus")
    
    # åœ¨é¡µé¢é¡¶éƒ¨åˆ›å»ºçŠ¶æ€å®¹å™¨
    status_container = st.empty()
    progress_bar = st.progress(0)
    status_container.info(f"å¼€å§‹åŠ è½½æ¨¡å‹: {model_name}")
    
    try:
        # åˆ†é˜¶æ®µé¢„åŠ è½½
        stages = ["pull", "load", "warmup"]
        for i, stage in enumerate(stages):
            progress = int((i / len(stages)) * 100)
            progress_bar.progress(progress)
            
            if stage == "pull":
                status_container.info(f"ä¸‹è½½æ¨¡å‹ {model_name}...")
                resp = requests.post(
                    f"{os.getenv('OLLAMA_API_BASE', 'http://localhost:11434')}/api/pull",
                    json={"name": model_name},
                    stream=True
                )
                # å¤„ç†ä¸‹è½½è¿›åº¦
                for _ in resp.iter_lines():
                    progress = min(progress + 2, 33)
                    progress_bar.progress(progress)
                    
            elif stage == "load":
                status_container.info(f"åŠ è½½æ¨¡å‹åˆ°å†…å­˜...")
                resp = requests.post(
                    f"{os.getenv('OLLAMA_API_BASE', 'http://localhost:11434')}/api/generate",
                    json={"model": model_name, "prompt": " "},
                    timeout=120
                )
                progress_bar.progress(66)
                
            else:  # warmup
                status_container.info(f"é¢„çƒ­æ¨¡å‹...")
                warmup_ollama_model()
                progress_bar.progress(100)
                
        status_container.success(f"{model_name} æ¨¡å‹å·²å°±ç»ªï¼")
        
    except Exception as e:
        progress_bar.progress(0)
        status_container.error(f"åŠ è½½å¤±è´¥: {str(e)}")
        raise
    finally:
        time.sleep(2)
        progress_bar.empty()
        status_container.empty()

def check_ollama_service() -> bool:
    """å¢å¼ºç‰ˆæ¨¡å‹æ£€æŸ¥"""
    try:
        # 1. æ£€æŸ¥æœåŠ¡å¯ç”¨æ€§
        resp = requests.get(f"{os.getenv('OLLAMA_API_BASE')}/api/tags", timeout=10)
        if resp.status_code != 200:
            logger.error(f"OllamaæœåŠ¡ä¸å¯ç”¨ (HTTP {resp.status_code})")
            return False
            
        # 2. æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
        active_model = os.getenv('OLLAMA_MODEL', 'phi4-mini-reasoning')
        models = [m['name'] for m in resp.json().get('models', [])]
        
        if active_model not in models:
            logger.error(f"æ¨¡å‹ {active_model} æœªåŠ è½½ï¼Œå¯ç”¨æ¨¡å‹: {', '.join(models)}")
            
            # å°è¯•è‡ªåŠ¨æ‹‰å–æ¨¡å‹
            pull_resp = requests.post(
                f"{os.getenv('OLLAMA_API_BASE')}/api/pull",
                json={"name": active_model},
                timeout=300
            )
            
            if pull_resp.status_code == 200:
                logger.info(f"æ­£åœ¨ä¸‹è½½æ¨¡å‹ {active_model}...")
                return True
            else:
                logger.error(f"æ¨¡å‹ {active_model} ä¸‹è½½å¤±è´¥")
                return False
                
        return True
        
    except Exception as e:
        logger.error(f"æ¨¡å‹æ£€æŸ¥å¤±è´¥: {str(e)}")
        return False

def process_pdf(file) -> dict:
    """å¸¦è¯¦ç»†è¯Šæ–­çš„PDFå¤„ç†"""
    status = st.empty()
    progress = st.progress(0)
    logger.info(f"å¼€å§‹å¤„ç†PDF: {file.name} ({file.size/1024:.1f}KB)")
    
    try:
        # é˜¶æ®µ1: è¯»å–æ–‡ä»¶å†…å®¹
        status.info("ğŸ“‚ è¯»å–PDFæ–‡ä»¶å†…å®¹...")
        progress.progress(10)
        
        # æ­£ç¡®è¯»å–Streamlit UploadedFileå¯¹è±¡
        contents = file.getvalue()
        progress.progress(30)
        
        # é˜¶æ®µ2: è§£æPDF
        status.info("ğŸ” è§£æPDFç»“æ„...")
        progress.progress(35)
        pdf_reader = PdfReader(BytesIO(contents))
        page_count = len(pdf_reader.pages)
        logger.info(f"å‘ç° {page_count} é¡µ")
        
        # é˜¶æ®µ3: æå–æ–‡æœ¬
        status.info(f"ğŸ“ æå–æ–‡æœ¬(å…±{page_count}é¡µ)...")
        text_pages = []
        for i, page in enumerate(pdf_reader.pages):
            text = page.extract_text() or ""
            text_pages.append(text)
            progress.progress(35 + int((i+1)/page_count*60))
            if i % 5 == 0:  # æ¯5é¡µè®°å½•ä¸€æ¬¡
                logger.debug(f"å·²å¤„ç† {i+1}/{page_count} é¡µï¼Œå½“å‰é¡µé•¿åº¦: {len(text)}å­—ç¬¦")
        
        full_text = "\n".join(text_pages)
        progress.progress(100)
        status.success(f"âœ… PDFå¤„ç†å®Œæˆï¼å…±æå– {len(full_text)} å­—ç¬¦")
        logger.info(f"PDFå¤„ç†å®Œæˆï¼Œæ€»å­—ç¬¦æ•°: {len(full_text)}")
        
        return {"text": full_text, "pages": page_count}
        
    except Exception as e:
        progress.progress(0)
        status.error(f"âŒ PDFå¤„ç†å¤±è´¥: {str(e)}")
        logger.error(f"PDFå¤„ç†é”™è¯¯: {str(e)}", exc_info=True)
        raise
    finally:
        time.sleep(2)
        progress.empty()
        status.empty()

def process_chunk_parallel(chunk: str, chunk_id: int, provider: str) -> tuple[int, str]:
    """çº¿ç¨‹å®‰å…¨çš„å—å¤„ç†"""
    try:
        model_manager.ensure_model_loaded(provider)
        return (chunk_id, generate_summary(chunk, level=1, provider=provider))
    except Exception as e:
        logger.error(f"åˆ†å—å¤„ç†å¤±è´¥: {str(e)}")
        return (chunk_id, f"[å¤„ç†å¤±è´¥: {str(e)}]")

def get_optimal_chunk_size(text: str) -> int:
    """åŠ¨æ€è®¡ç®—æœ€ä½³åˆ†å—å¤§å°"""
    # æŠ€æœ¯æ–‡æ¡£ç‰¹å¾æ£€æµ‹
    if any(keyword in text[:1000] for keyword in 
           ['Abstract', 'å¼•è¨€', 'å®éªŒæ–¹æ³•', 'å‚è€ƒæ–‡çŒ®']):
        return 4000  # æŠ€æœ¯æ–‡æ¡£ç”¨å°åˆ†å—
    
    # å°è¯´/æ•£æ–‡æ£€æµ‹
    if any(keyword in text[:1000] for keyword in 
           ['ç¬¬[ä¸€äºŒä¸‰å››]ç« ', 'CHAPTER', '......']):
        return 8000  # æ–‡å­¦ä½œå“ç”¨å¤§åˆ†å—
        
    return 6000  # é»˜è®¤å€¼

def split_into_chunks(text: str) -> list[str]:
    """æ™ºèƒ½åˆ†å—å‡½æ•°"""
    CHUNK_SIZE = get_optimal_chunk_size(text)
    
    # æŒ‰æ®µè½åˆ†å—(ä¿æŒè¯­ä¹‰å®Œæ•´)
    paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
    chunks = []
    current_chunk = ""
    
    for para in paragraphs:
        if len(current_chunk) + len(para) > CHUNK_SIZE and current_chunk:
            chunks.append(current_chunk)
            current_chunk = para
        else:
            current_chunk += "\n" + para
    
    if current_chunk:
        chunks.append(current_chunk)
    
    logger.info(f"å°†æ–‡æœ¬åˆ†æˆ {len(chunks)} ä¸ªå—ï¼Œå¹³å‡å¤§å°: {sum(len(c) for c in chunks)//len(chunks)}å­—ç¬¦")
    return chunks

def generate_hierarchical_summary(text: str, provider: str) -> dict:
    """å®Œæ•´çš„åˆ†å±‚æ‘˜è¦å¤„ç†(å¸¦æš‚åœåŠŸèƒ½)"""
    # åˆå§‹åŒ–æ§åˆ¶é¢æ¿
    control_placeholder = st.empty()
    with control_placeholder.container():
        st.markdown("**å¤„ç†æ§åˆ¶**")
        col1, col2 = st.columns(2)
        with col1:
            pause_btn = st.button("â¸ï¸ æš‚åœ", key="pause_btn")
        with col2:
            resume_btn = st.button("â–¶ï¸ ç»§ç»­", key="resume_btn", disabled=True)
    
    # åˆå§‹åŒ–è¿›åº¦ç»„ä»¶
    progress_bar = st.progress(0)
    status_text = st.empty()
    details = st.expander("å¤„ç†è¯¦æƒ…", expanded=True)
    
    # å¤„ç†çŠ¶æ€
    result = {'chunk_summaries': [], 'section_summaries': [], 'final_summary': "", 'errors': []}
    
    try:
        # é˜¶æ®µ1: é¢„å¤„ç†
        status_text.markdown("**é˜¶æ®µ1/4**: é¢„å¤„ç†æ–‡æœ¬...")
        text = preprocess_text(text)
        progress_bar.progress(5)
        check_pause(pause_btn, resume_btn, control_placeholder)
        
        # é˜¶æ®µ2: åˆ†å—å¤„ç†
        status_text.markdown("**é˜¶æ®µ2/4**: åˆ†å—å¤„ç†ä¸­...")
        chunks = split_into_chunks(text)
        
        with details:
            chunk_status = st.empty()
            
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = []
            for i, chunk in enumerate(chunks):
                check_pause(pause_btn, resume_btn, control_placeholder)
                with details:
                    chunk_status.write(f"ğŸ”„ å¤„ç†åˆ†å— {i+1}/{len(chunks)}")
                futures.append(executor.submit(process_chunk_parallel, chunk, i, provider))
                progress_bar.progress(5 + int(45*(i+1)/len(chunks)))
            
            for future in concurrent.futures.as_completed(futures):
                check_pause(pause_btn, resume_btn, control_placeholder)
                chunk_id, summary = future.result()
                result['chunk_summaries'].append((chunk_id, summary))
        
        # é˜¶æ®µ3: ç« èŠ‚æ‘˜è¦
        status_text.markdown("**é˜¶æ®µ3/4**: ç”Ÿæˆç« èŠ‚æ‘˜è¦...")
        check_pause(pause_btn, resume_btn, control_placeholder)
        
        # [ç« èŠ‚å¤„ç†é€»è¾‘]
        progress_bar.progress(80)
        
        # é˜¶æ®µ4: æœ€ç»ˆæ‘˜è¦
        status_text.markdown("**é˜¶æ®µ4/4**: ç”Ÿæˆæœ€ç»ˆæ‘˜è¦...")
        check_pause(pause_btn, resume_btn, control_placeholder)
        
        result['final_summary'] = generate_final_summary(result['section_summaries'])
        progress_bar.progress(100)
        status_text.success("âœ… å¤„ç†å®Œæˆï¼")
        
    except Exception as e:
        progress_bar.progress(100)
        status_text.error(f"âŒ å¤„ç†å¤±è´¥: {str(e)}")
        result['errors'].append(str(e))
    
    return result

def check_pause(pause_btn: bool, resume_btn: bool, placeholder):
    """æ£€æŸ¥å¹¶å¤„ç†æš‚åœçŠ¶æ€"""
    if pause_btn:
        with placeholder.container():
            st.session_state.paused = True
            st.button("â–¶ï¸ ç»§ç»­", key="resume_btn")
        
        while st.session_state.paused:
            time.sleep(0.5)
            if resume_btn:
                st.session_state.paused = False
                break

def preprocess_text(text: str) -> str:
    """ç»ˆææ–‡æœ¬é¢„å¤„ç†"""
    # ç§»é™¤PDFå¸¸è§å™ªå£°
    patterns = [
        r'\d{1,3}\s+[\u4e00-\u9fa5]+\s+\d{1,3}',  # é¡µçœ‰é¡µç 
        r'Â©.*\d{4}',  # ç‰ˆæƒä¿¡æ¯
        r'http[s]?://\S+',  # URLé“¾æ¥
        r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'  # é‚®ç®±
    ]
    
    for pattern in patterns:
        text = re.sub(pattern, '', text)
    
    # æ™ºèƒ½æ®µè½è¿‡æ»¤
    lines = []
    for line in text.split('\n'):
        line = line.strip()
        if not line:
            continue
            
        # ä¿ç•™æœ‰æ•ˆæ®µè½
        if len(line) > 25 or \
           any(c.isalpha() for c in line) or \
           line.endswith(('ã€‚', '!', '?', ';')):
            lines.append(line)
    
    return '\n'.join(lines)

def generate_final_summary(section_summaries: list[str]) -> str:
    """ç”Ÿæˆæœ€ç»ˆæ‘˜è¦"""
    if not section_summaries:
        return "[æ— æœ‰æ•ˆæ‘˜è¦å†…å®¹]"
    
    # åˆå¹¶æ‰€æœ‰ç« èŠ‚æ‘˜è¦
    combined = "\n\n".join(section_summaries)
    
    try:
        # è°ƒç”¨Ollamaç”Ÿæˆæœ€ç»ˆæ‘˜è¦
        response = requests.post(
            f"{os.getenv('OLLAMA_API_BASE')}/api/generate",
            json={
                "model": os.getenv("OLLAMA_MODEL"),
                "prompt": f"è¯·æ ¹æ®ä»¥ä¸‹ç« èŠ‚æ‘˜è¦ï¼Œç”¨ä¸­æ–‡ç”Ÿæˆä¸€ä¸ªç»“æ„åŒ–çš„æœ€ç»ˆæ€»ç»“(500å­—ä»¥å†…):\n\n{combined}",
                "options": {"temperature": 0.3, "num_ctx": 4096}
            },
            timeout=120
        )
        
        if response.status_code == 200:
            result = ""
            for line in response.iter_lines():
                if line:
                    data = json.loads(line)
                    result += data.get("response", "")
                    if data.get("done", False):
                        break
            return result.strip()
        else:
            return "[æ‘˜è¦ç”Ÿæˆå¤±è´¥]"
    except Exception as e:
        logger.error(f"æœ€ç»ˆæ‘˜è¦ç”Ÿæˆé”™è¯¯: {str(e)}")
        return "[æ‘˜è¦ç”Ÿæˆå¼‚å¸¸]"

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'summaries' not in st.session_state:
    st.session_state.summaries = {}
    
if 'full_text' not in st.session_state:
    st.session_state.full_text = ""
    
if 'current_level' not in st.session_state:
    st.session_state.current_level = 1

# ä¸»ç•Œé¢
st.title("PDFå¿«é€Ÿé˜…è¯»åŠ©æ‰‹")
st.subheader("å±‚æ¬¡åŒ–æ‘˜è¦å·¥å…·")

# ä¾§è¾¹æ è®¾ç½®
with st.sidebar:
    model_provider = st.selectbox(
        "é€‰æ‹©æ‘˜è¦æ¨¡å‹",
        ["OpenAI", "DeepSeek", "Ollama"],
        key="sidebar_model_provider"
    )
    
    setup_model_management()

# PDFä¸Šä¼ 
uploaded_file = st.file_uploader("ä¸Šä¼ PDFæ–‡ä»¶", type="pdf")

if uploaded_file:
    try:
        # ä½¿ç”¨å•ä¸€çŠ¶æ€å®¹å™¨
        status_container = st.empty()
        progress_bar = st.progress(0)
        
        status_container.info("å¼€å§‹å¤„ç†PDF...")
        progress_bar.progress(10)
        
        # 1. è§£æPDF
        pdf_data = process_pdf(uploaded_file)
        progress_bar.progress(30)
        
        # 2. åˆ†å±‚æ‘˜è¦
        status_container.info("å¼€å§‹åˆ†å±‚æ‘˜è¦...")
        result = generate_hierarchical_summary(
            text=pdf_data['text'],
            provider=model_provider
        )
        progress_bar.progress(90)
        
        # ä¿å­˜ç»“æœ
        st.session_state["full_text"] = pdf_data["text"]
        st.session_state["chunk_summaries"] = result["chunk_summaries"]
        st.session_state["section_summaries"] = result["section_summaries"]
        st.session_state["final_summary"] = result["final_summary"]
        
        progress_bar.progress(100)
        status_container.success("å¤„ç†å®Œæˆï¼")
        
        # æ˜¾ç¤ºç»“æœå¯¼èˆª
        tab1, tab2, tab3 = st.tabs(["è¯¦ç»†æ‘˜è¦", "ç« èŠ‚æ‘˜è¦", "æœ€ç»ˆæ‘˜è¦"])
        
        with tab1:
            for i, summary in enumerate(st.session_state["chunk_summaries"]):
                with st.expander(f"åˆ†å— {i+1}", expanded=(i<3)):
                    st.write(summary)
        
        with tab2:
            for i, summary in enumerate(st.session_state["section_summaries"]):
                st.markdown(f"### ç« èŠ‚ {i+1}")
                st.write(summary)
        
        with tab3:
            st.write(st.session_state["final_summary"])
            
    except Exception as e:
        st.error(f"å¤„ç†å¤±è´¥: {str(e)}")
        logger.error(f"å¤„ç†é”™è¯¯: {str(e)}", exc_info=True)
    finally:
        time.sleep(2)
        progress_bar.empty()
        status_container.empty()

# å¯¼èˆªæ§åˆ¶
col1, col2 = st.columns(2)
with col1:
    if st.button("â† ä¸Šä¸€å±‚çº§") and st.session_state.current_level > 1:
        st.session_state.current_level -= 1
        st.experimental_rerun()
with col2:
    if st.button("é‡ç½®åˆ°ç¬¬ä¸€å±‚"):
        st.session_state.current_level = 1
        st.experimental_rerun()
